{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimization_SGD.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPaaMOV9JoXBv31a27L/hHM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vM6mcohXM25x","executionInfo":{"status":"ok","timestamp":1618365452186,"user_tz":-480,"elapsed":56432,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}},"outputId":"6579e3eb-51b7-47cf-de33-5c1b57bc01b3"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive\n","import os\n","os.chdir(\"drive/Colab\") "],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 160983 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.24-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.24-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6LFQfD-Dr_4d","executionInfo":{"status":"ok","timestamp":1618365471727,"user_tz":-480,"elapsed":16891,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}}},"source":["import pickle\n","def unpickle(file):\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","import numpy as np\n","data = unpickle('./Data/cifar-10-batches-py/data_batch_1')\n","Xtr = data[b'data']\n","Ytr = data[b'labels']\n","data = unpickle('./Data/cifar-10-batches-py/data_batch_2')\n","Xtr = np.vstack((Xtr,data[b'data']))\n","Ytr = np.hstack((Ytr,data[b'labels']))\n","data = unpickle('./Data/cifar-10-batches-py/data_batch_3')\n","Xtr = np.vstack((Xtr,data[b'data'])) \n","Ytr = np.hstack((Ytr,data[b'labels']))\n","data = unpickle('./Data/cifar-10-batches-py/data_batch_4')\n","Xtr = np.vstack((Xtr,data[b'data'])) \n","Ytr = np.hstack((Ytr,data[b'labels']))\n","data = unpickle('./Data/cifar-10-batches-py/data_batch_5')\n","Xtr = np.vstack((Xtr,data[b'data']))\n","Ytr = np.hstack((Ytr,data[b'labels']))\n","data = unpickle('./Data/cifar-10-batches-py/test_batch')\n","Xte = data[b'data']\n","Yte = data[b'labels']\n","# flatten out all images to be one-dimensional\n","Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\n","Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-w6CWjiKr3iJ","executionInfo":{"status":"ok","timestamp":1618365504608,"user_tz":-480,"elapsed":807,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}}},"source":["def L_i(x, y, W):\n","  \"\"\"\n","  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n","  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n","    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n","  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n","  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n","  \"\"\"\n","  delta = 1.0 # see notes about delta later in this section\n","  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n","  correct_class_score = scores[y]\n","  D = W.shape[0] # number of classes, e.g. 10\n","  loss_i = 0.0\n","  for j in range(D): # iterate over all wrong classes\n","    if j == y:\n","      # skip for the true class to only loop over incorrect classes\n","      continue\n","    # accumulate loss for the i-th example\n","    loss_i += max(0, scores[j] - correct_class_score + delta)\n","  return loss_i\n","\n","def L_i_vectorized(x, y, W):\n","  \"\"\"\n","  A faster half-vectorized implementation. half-vectorized\n","  refers to the fact that for a single example the implementation contains\n","  no for loops, but there is still one loop over the examples (outside this function)\n","  \"\"\"\n","  delta = 1.0\n","  scores = W.dot(x)\n","  # compute the margins for all classes in one vector operation\n","  margins = np.maximum(0, scores - scores[y] + delta)\n","  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n","  # to ignore the y-th position and only consider margin on max wrong class\n","  margins[y] = 0\n","  loss_i = np.sum(margins)\n","  return loss_i\n","\n","def L(X, y, W):\n","  \"\"\"\n","  fully-vectorized implementation :\n","  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n","  - y is array of integers specifying correct class (e.g. 50,000-D array)\n","  - W are weights (e.g. 10 x 3073)\n","  \"\"\"\n","  # evaluate loss over all examples in X without using any for loops\n","  # left as exercise to reader in the assignment\n","  delta = 1.0\n","  scores = W.dot(X)\n","  # compute the margins for all classes in one vector operation\n","  margins = np.maximum(0, scores - scores[y,[np.arange(X.shape[1])]] + delta)\n","  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n","  # to ignore the y-th position and only consider margin on max wrong class\n","  margins[y,[np.arange(scores.shape[1])]] = 0\n","  # loss_i = np.sum(margins,axis=0)\n","  loss_i = np.sum(margins)/margins.shape[1]\n","  return loss_i"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxLjWnhgGhWe","executionInfo":{"status":"ok","timestamp":1618365523677,"user_tz":-480,"elapsed":2439,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}}},"source":["# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n","X_train = np.hstack((np.array(Xtr_rows), np.ones((50000,1)))).T.copy()\n","# assume Y_train are the labels (e.g. 1D array of 50,000)\n","Y_train = Ytr.copy()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Elp3wvrhNFQA"},"source":["# assume the function L evaluates the loss function\n","\n","bestloss = float(\"inf\") # Python assigns the highest possible float value\n","for num in range(300):\n","  W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n","  loss = L(X_train, Y_train, W) # get the loss over the entire training set\n","  if loss < bestloss: # keep track of the best solution\n","    bestloss = loss\n","    bestW = W\n","  print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))\n","\n","# prints:\n","# in attempt 0 the loss was 9.401632, best 9.401632\n","# in attempt 1 the loss was 8.959668, best 8.959668\n","# in attempt 2 the loss was 9.044034, best 8.959668\n","# in attempt 3 the loss was 9.278948, best 8.959668\n","# in attempt 4 the loss was 8.857370, best 8.857370\n","# in attempt 5 the loss was 8.943151, best 8.857370\n","# in attempt 6 the loss was 8.605604, best 8.605604\n","# ... (trunctated: continues for 1000 lines)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gn4n63riG67t","executionInfo":{"status":"ok","timestamp":1618365695207,"user_tz":-480,"elapsed":823,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}}},"source":["# Assume X_test is [3073 x 10000], Y_test [10000 x 1]\n","X_test = np.hstack((np.array(Xte_rows), np.ones((10000,1)))).T.copy()\n","# assume Y_train are the labels (e.g. 1D array of 50,000)\n","Y_test = Yte.copy()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ah8f8GCYGW-h","executionInfo":{"status":"ok","timestamp":1618366369940,"user_tz":-480,"elapsed":726,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}},"outputId":"0a64d03b-18dd-4921-e9d0-dc51c394bd54"},"source":["scores = bestW.dot(X_test) # 10 x 10000, the class scores for all test examples\n","# find the index with max score in each column (the predicted class)\n","Yte_predict = np.argmax(scores, axis = 0)\n","# and calculate accuracy (fraction of predictions that are correct)\n","np.mean(Yte_predict == Yte)\n","# returns 0.1555"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1113"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"7-LPPRV6GZ9Q"},"source":["# W = np.random.randn(10, 3073) * 0.001 # generate random starting W\n","# bestloss = float(\"inf\")\n","for i in range(10000):\n","  step_size = 0.00001\n","  Wtry = W + np.random.randn(10, 3073) * step_size\n","  loss = L(X_train, Y_train, Wtry)\n","  if loss < bestloss:\n","    W = Wtry\n","    bestloss = loss\n","  print('iter %d loss is %f' % (i, bestloss))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTxuoPesQlLI","executionInfo":{"status":"ok","timestamp":1618375875000,"user_tz":-480,"elapsed":1587,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}},"outputId":"0b55fe3e-e9fa-4729-a0eb-0bdf0840d7b3"},"source":["scores = W.dot(X_test) # 10 x 10000, the class scores for all test examples\n","# find the index with max score in each column (the predicted class)\n","Yte_predict = np.argmax(scores, axis = 0)\n","# and calculate accuracy (fraction of predictions that are correct)\n","np.mean(Yte_predict == Yte)\n","# returns 0.1555"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2469"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"_IjMv0mfGcEL","executionInfo":{"status":"ok","timestamp":1618376116150,"user_tz":-480,"elapsed":818,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}}},"source":["def eval_numerical_gradient(f, x):\n","  \"\"\"\n","  a naive implementation of numerical gradient of f at x\n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\"\n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros(x.shape)\n","  h = 0.00001\n","\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    old_value = x[ix]\n","    x[ix] = old_value + h # increment by h\n","    fxh = f(x) # evalute f(x + h)\n","    x[ix] = old_value - h # increment by h\n","    fxh_ = f(x) # evalute f(x - h)\n","    x[ix] = old_value # restore to previous value (very important!)\n","\n","    # compute the partial derivative\n","    grad[ix] = (fxh - fxh_) / (2 * h) # the slope\n","    it.iternext() # step to next dimension\n","\n","  return grad"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPy2wQ2mGe0g"},"source":["# to use the generic code above we want a function that takes a single argument\n","# (the weights in our case) so we close over X_train and Y_train\n","def CIFAR10_loss_fun(W):\n","  return L(X_train, Y_train, W)\n","\n","W = np.random.rand(10, 3073) * 0.001 # random weight vector\n","df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vm2YfhmpGhCc"},"source":["loss_original = CIFAR10_loss_fun(W) # the original loss\n","print('original loss: %f' % (loss_original, ))\n","\n","# lets see the effect of multiple step sizes\n","for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n","  step_size = 10 ** step_size_log\n","  W_new = W - step_size * df # new position in the weight space\n","  loss_new = CIFAR10_loss_fun(W_new)\n","  print('for step size %f new loss: %f' % (step_size, loss_new))\n","\n","# prints:\n","# original loss: 2.200718\n","# for step size 1.000000e-10 new loss: 2.200652\n","# for step size 1.000000e-09 new loss: 2.200057\n","# for step size 1.000000e-08 new loss: 2.194116\n","# for step size 1.000000e-07 new loss: 2.135493\n","# for step size 1.000000e-06 new loss: 1.647802\n","# for step size 1.000000e-05 new loss: 2.844355\n","# for step size 1.000000e-04 new loss: 25.558142\n","# for step size 1.000000e-03 new loss: 254.086573\n","# for step size 1.000000e-02 new loss: 2539.370888\n","# for step size 1.000000e-01 new loss: 25392.214036"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ih8v3XB_GlzP"},"source":["# Vanilla Gradient Descent\n","\n","while True:\n","  weights_grad = evaluate_gradient(loss_fun, data, weights)\n","  weights += - step_size * weights_grad # perform parameter update"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHkzmw-rGmim"},"source":["# Vanilla Minibatch Gradient Descent\n","\n","while True:\n","  data_batch = sample_training_data(data, 256) # sample 256 examples\n","  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n","  weights += - step_size * weights_grad # perform parameter update"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvlZ3io4GoXa","executionInfo":{"status":"ok","timestamp":1618304999923,"user_tz":-480,"elapsed":2078,"user":{"displayName":"李天骥","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhahgMPaXCKIqlV8NSnE7KsQqTL9lPfNSxbMjNymg=s64","userId":"01019973527312377085"}},"outputId":"b7e2dece-03b8-475d-cac3-b32b8ec14f69"},"source":["import numpy as np\n"," \n","a = np.arange(0,60,5) \n","a = a.reshape(3,4)  \n","print ('原始数组是：')\n","print (a)\n","print ('\\n')\n","for x in np.nditer(a, op_flags=['readwrite']): \n","    x[...]=2*x \n","print ('修改后的数组是：')\n","print (a)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["原始数组是：\n","[[ 0  5 10 15]\n"," [20 25 30 35]\n"," [40 45 50 55]]\n","\n","\n","修改后的数组是：\n","[[  0  10  20  30]\n"," [ 40  50  60  70]\n"," [ 80  90 100 110]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ix44__C3fl4X"},"source":[""],"execution_count":null,"outputs":[]}]}